{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5d03bc",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88bda4",
   "metadata": {},
   "source": [
    "## The problem with modeling Long sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9017ea",
   "metadata": {},
   "source": [
    "Before we dive into the self-attention mechanism at the heart of LLMS, let's consider the problem with pre-LLM architectures that do not include attention mechanisms.\n",
    "\n",
    "Suppose we want to develop a language translation model that translates text from one language to another. We can't simply translate a text word by word due to grammatical structures in the source and target language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6e024",
   "metadata": {},
   "source": [
    "To address this problem, it is common to use a **deep neural network** with two submodules, an *encoder and a decoder*.\n",
    "\n",
    "- Encoder: Read in and process the entire text\n",
    "- Decoder: Produces the translated text\n",
    "\n",
    "Before the advent of transformers, recurrent reural networks (RNNs) wer the most popular encoder-decoder architecture for language translation.\n",
    "\n",
    "RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text\n",
    "\n",
    "**RNNs:**\n",
    "- RNNs process sequences step-by-step, feeding the output of one step into the next\n",
    "- In translation:\n",
    "    - The encoder processes each word sequentially, updating its internal hidden state\n",
    "    - The final hidden state summarizes the whole input sentence\n",
    "    - The decoder uses thisfinal state to generate the translated text one word at a time\n",
    "\n",
    "**Problem:**\n",
    "- RNNs can't easily access earlier words once they have been processed\n",
    "- They rely only on the final hidden state, which compresses all sentence information into one vector\n",
    "- This causes loss of context\n",
    "\n",
    "**Why Attention Was Invented:**\n",
    "- Because RNNs struggled to retain long-term dependencies, researchers created attention mechanisms\n",
    "- Attention allows the model to look back at all words in the input sequence - not just the last one - to make better, context-aware predictions\n",
    "\n",
    "![](pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54ef3b",
   "metadata": {},
   "source": [
    "## Attending to different parts of the input with self-attention\n",
    "\n",
    "*Learning and coding the self-attention mechanism from scratch*\n",
    "\n",
    "**The \"SELF\" in SELF-ATTENTION**\n",
    "- The \"self\" means the model is paying attention to itself - to different parts of the same input sequence\n",
    "- It learns how each token relates to every other token in that same sequence\n",
    "- So, each word gets a context-aware representation that blends information from all other words in that sequence\n",
    "\n",
    "### Simple self-attention mechanism without trainable weights\n",
    "- Context vectors play a crucial role in self-attention\n",
    "- Their purpose is to create enriched representations of each element in an input sequence  by incorporating information from all other elements in the sequence\n",
    "- This is essential in LLMs, which need to understand the relationship and relevance of words in a sequence to each other\n",
    "- Later, we will add trainable weights that help an LLM learn to construct these context vectors swo that they are relevant for the LLM to generate the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe484e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create embedding dimensions\n",
    "inputs = torch.tensor([[0.43,0.15,0.89],\n",
    "                     [0.55,0.87,0.66],\n",
    "                     [0.57,0.85,0.64],\n",
    "                     [0.22,0.58,0.33],\n",
    "                     [0.77,0.25,0.10],\n",
    "                     [0.05,0.8,0.55]])\n",
    "\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i,x_i  in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i,query)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7265b",
   "metadata": {},
   "source": [
    "- Attention is a way for model to decide which parts of the input matter most for what it is doing right now\n",
    "\n",
    "**Core Idea**\n",
    "When processing a token, the model looks back at all tokens and assigns each a weight: pay a lot of attention to these, less to those. It then mixes information from all tokens using those weights to get a focused summary -> a context vector\n",
    "\n",
    "**Self-attention:** Tokens attend to other tokens in the same sequence (used in Transformers to let every word see every other word)\n",
    "\n",
    "**Cross-attention:** One sequence attends to another (e.g., decoder attending to encoder outputs in translation, or text attending to image features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3733818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(attn_weights_2_tmp)\n",
    "print(attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba30bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Turns a list of arbitrary scores into a probability distribution\n",
    "# It is used in attention to convert similarity scores into attention weights that tell how much to focus on each token\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()\n",
    "\n",
    "\n",
    "attn_weights_naive = softmax_naive(attn_scores_2)\n",
    "print(attn_weights_naive)\n",
    "print(attn_weights_naive.sum())\n",
    "\n",
    "# Softmax function ensures that the attention weights are always positivie\n",
    "# This makes the output interpretable as probabilities or relative importance, where\n",
    "# higher weights mean more focus on that token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9aba3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(attn_weights_2)   \n",
    "print(attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad75361",
   "metadata": {},
   "source": [
    "- After computing attention weights for each token -> we now build one summary vector - context vector by:\n",
    "    1. Taking each token's embedding\n",
    "    2. Multiplying it by its attention weight\n",
    "    3. Adding them all up\n",
    "\n",
    "- Query: What am I looking for?\n",
    "    - A vector made from the current token. It encodes the need of this token - what information it wants from others\n",
    "\n",
    "- Key: What do I contain?\n",
    "    - A vector made from each token in a sequence. It summarizes what that token can offer to others\n",
    "\n",
    "- How they are used:\n",
    "    - For the current token, we compare its Query to every other token's Key. Big Q.K => That token is relevant. We softmax these similarities to get attention weights. Then we use those weights to mix the token's values into a single context vector for the current token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6598cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccbb74",
   "metadata": {},
   "source": [
    "## Computing Attention Weights for all input tokens\n",
    "\n",
    "![](pic2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8cf363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f4c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize each row of attention scores to get attention weights\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22bc00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Use these attention weights to compute all context vectors via matrix multiplication\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4eff2b",
   "metadata": {},
   "source": [
    "## Implementing self-attention with trainable weights\n",
    "\n",
    "- Scaled dot-product attention\n",
    "- The self-attention mechanism with trainable weights builds on the previous concepts: we want to compute context vectors as weighted sums over the input vectors specific to a certain input element\n",
    "- The most notable difference compared to the previous part is the introduction of weight matrices that are updated during model training\n",
    "- These trainable wieght matrices are crucial so that the model can learn to produdce \"good\" context vectors --> train the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0381b7e",
   "metadata": {},
   "source": [
    "### Computing the attention weights step by step\n",
    "- Introduce 3 trainable weight matrices:\n",
    "    1. W_k\n",
    "    2. W_q\n",
    "    3. W_v\n",
    "- These matrices are used to project the embedded input tokens into query, key, and value vectors\n",
    "\n",
    "![](pic3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0691c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 \n",
    "\n",
    "torch.manual_seed(123)\n",
    "# Initialize weight matrices\n",
    "# Use requires_grad=False to indicate these are not trainable in this example\n",
    "# In practice, these would be trainable parameters -> use requires_grad=True\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7724c5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector: tensor([-1.1729, -0.0048])\n",
      "Key vector: tensor([-0.1142, -0.7676])\n",
      "Value vector: tensor([0.4107, 0.6274])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key       \n",
    "value_2 = x_2 @ W_value\n",
    "print(\"Query vector:\", query_2)\n",
    "print(\"Key vector:\", key_2)\n",
    "print(\"Value vector:\", value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf9df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd00dc",
   "metadata": {},
   "source": [
    "![](pic%204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff1592",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f33dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        queries = x @ self.W_query\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[1]**0.5, dim=-1)\n",
    "\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da3e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2845, 0.4071],\n",
      "        [0.2854, 0.4081],\n",
      "        [0.2854, 0.4075],\n",
      "        [0.2864, 0.3974],\n",
      "        [0.2863, 0.3910],\n",
      "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "\n",
    "print(sa_v1.forward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df926e4b",
   "metadata": {},
   "source": [
    "We can improve the *SelfAttention_v1* implementation further by utilizing PyTorch's nn.linear layers, which effectively perform matrix multiplication wehn the bias units are disabled.\n",
    "\n",
    "nn.Linear has an **optimized weight** initialization scheme, contributing to more stable and effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2b732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# nn.linear: Fully connected (dense) layer that performs linear transformation\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[1]**0.5, dim=-1)\n",
    "\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "108831c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd7df5",
   "metadata": {},
   "source": [
    "1. Casual Attention\n",
    "This means the model can't peek into the future. When predicting the next word, it should only look at the previous words, not the ones that come after\n",
    "\n",
    "2. Multi-head attention\n",
    "Instead of one big attention calculation, the model uses multiple smaller attention layers (called \"heads\"). Each head learns to focus on different parts of the sentence or different kinds of relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878edf2",
   "metadata": {},
   "source": [
    "## Hiding Future Words with Casual Attention\n",
    "\n",
    "For many LLM task, we want the self-attention mechanism to consider only the tokens that appear prior to the current position when predicting the next token in a sequence -> Use casual attention\n",
    "\n",
    "It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once\n",
    "\n",
    "![](pic5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10deb87",
   "metadata": {},
   "source": [
    "### Applying a casual Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "440942fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b9a71e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a mask where values above the diagonal are zero\n",
    "# torch.tril(input, diagonal=0,*,out=None): Creates a lower triangular matrix\n",
    "# torch.ones(size,*,out=None,dtype=None,...): Creates a tensor filled entirely with 1s\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "ones_layer = torch.ones(context_length,context_length)\n",
    "print(ones_layer)\n",
    "print(\"\\n\")\n",
    "mask_simple = torch.tril(ones_layer)\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a62c661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da85d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalize the attention weights to sum up to 1 again in each row\n",
    "row_sums = masked_simple.sum(dim=-1,keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a35a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implement more efficient mask by creating a mask with 1s above the diagonal and then replacing the 1s with negative infinity\n",
    "\n",
    "mask = torch.triu(ones_layer,diagonal = 1)\n",
    "print(mask)\n",
    "print(\"\\n\")\n",
    "masked = attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0f75fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e8ff8",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout\n",
    "\n",
    "**Dropout:** Is a technique where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out.\n",
    "\n",
    "This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units -> dropout is only used during training\n",
    "\n",
    "Dropout in attention mechnaism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "138a3b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9e89272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4925, 0.4638, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3941, 0.0000],\n",
      "        [0.3869, 0.3327, 0.0000, 0.3084, 0.3331, 0.3058]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa639a3",
   "metadata": {},
   "source": [
    "### Implementing a compact casual attention class\n",
    "\n",
    "Incorporate the casual attention and dropout modifications into the SelfAttention Python Class -> serve as a template for developing multi-head attention -> final attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48c559c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack() = joins a sequence of tensors along a new dimension -> adds a new axis and stacks tensors on top of each other like layers\n",
    "\n",
    "batch = torch.stack((inputs,inputs),dim=0)\n",
    "print(batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2be0fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length,dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Adds a tensor to your model that is not a learnable parameter but still gets saved with the model\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens],-torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b1e54d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8ffca",
   "metadata": {},
   "source": [
    "## Extending single-head attention to multi-head attention\n",
    "\n",
    "Dividing the attention mechanism into multiple \"heads,\" each operating independently. In this context, a single casual attention module can be considered single-attention, where there is only one set of attention weights processing the input sequentially\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c7d19",
   "metadata": {},
   "source": [
    "### Stacking multiple single-head attention layers\n",
    "\n",
    "Implementing multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights, and then combining their outputs\n",
    "\n",
    "![](pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efe82189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main idea of multi-head is to run attention mechanism multiple times in parallel\n",
    "# Achieve this by implementing a simple wrapper class that stacks multiple instances of CasualAttention class\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # nn.ModuleList: A list that can store nn.Module objects -> ensures all modules are properly registered and their parameters are tracked\n",
    "        # Designed to hold multiple layers in a way that Pytorch can track and train them\n",
    "        self.heads = nn.ModuleList([\n",
    "            CasualAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    # torch.cat(): concatenates tensors along an exisiting dimension\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c12409ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 8])\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729, -0.5684,\n",
      "           0.5063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011, -0.5388,\n",
      "           0.6447],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102, -0.5242,\n",
      "           0.6954],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785, -0.4578,\n",
      "           0.6471],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520, -0.4006,\n",
      "           0.5921],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499, -0.3997,\n",
      "           0.5971]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729, -0.5684,\n",
      "           0.5063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011, -0.5388,\n",
      "           0.6447],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102, -0.5242,\n",
      "           0.6954],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785, -0.4578,\n",
      "           0.6471],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520, -0.4006,\n",
      "           0.5921],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499, -0.3997,\n",
      "           0.5971]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # Number of tokens\n",
    "d_in, d_out = 3,2 # Input and output dimensions\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.0, num_heads=4)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs.shape)\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3e31a",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with weight splits\n",
    "\n",
    "In the new class, you won't need separate CasualAttention objects anymore.\n",
    "\n",
    "Instead, it will:\n",
    "1. Split the input into multiple heads (by reshaping the data)\n",
    "2. Compute attention for all heads in one go\n",
    "3. Merge the results back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445189df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias =False):\n",
    "        # Calls the constructor of the parent class nn.Module\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # Initialize weights and biases for query, key, and value projections\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Contain the combined results from all attention heads\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "\n",
    "            # torch.triu(input, diagonal=0,*,out=None): Creates a upper triangular matrix\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Compute queries, keys, and values\n",
    "        # Q = x @ W_query\n",
    "        # K = x @ W_key\n",
    "        # V = x @ W_value\n",
    "        # nn.Linear layers handle the matrix multiplications internally batch by batch\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape to (batch_size, num_tokens, num_heads, head_dim)\n",
    "        # .view(): Returns a new tensor with the same data as the original tensor but with a different shape\n",
    "        \"\"\"\n",
    "        batch\n",
    "            head 0\n",
    "                token 0 -> head_dim\n",
    "                token 1 -> head_dim\n",
    "                ...\n",
    "            \n",
    "            head 1\n",
    "                token 0 -> head_dim\n",
    "                token 1 -> head_dim\n",
    "                ...\n",
    "            ...\n",
    "        \n",
    "        \"\"\"\n",
    "        # input: (b, num_tokens, d_out)\n",
    "        # b: batch size\n",
    "        # num_tokens: number of tokens in the sequence\n",
    "        # d_out: output dimension - aka number of features per token after projection\n",
    "        # Before view: Each token has d_out features but these 8 must be split into num_heads heads, each of head_dim features\n",
    "\n",
    "        # .view(b, num_tokens, self.num_heads, self.head_dim): Reshape the tensor to have separate dimensions for heads\n",
    "        # .transpose(1,2): Swap the num_tokens and num_heads dimensions to facilitate attention computation\n",
    "\n",
    "        \"\"\"\n",
    "        Example: If a token embedding is: [v0,v1,v2,v3,v4,v5,v6,v7] and num_heads=2, head_dim=4\n",
    "        After view:\n",
    "        [v0,v1,v2,v3]  # head 0\n",
    "        [v4,v5,v6,v7]  # head 1\n",
    "        After transpose:\n",
    "        (batch, heads,tokens, head_dim) )\n",
    "        \"\"\"\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = attn_weights @ values\n",
    "\n",
    "        context_vecs = context_vecs.transpose(1,2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        output = self.out_proj(context_vecs)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef34c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multihead - rewrite - learn\n",
    "\n",
    "# class MultiHeadAttention_v3(nn.Module):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "#         super().__init__()\n",
    "#         assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "#         self.d_out = d_out\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_out // num_heads\n",
    "\n",
    "#         # Initialize weights and biases for QKV\n",
    "#         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "#         # Output projection layer\n",
    "#         self.out_proj = nn.Linear(d_out, d_out)\n",
    "#         # Dropout layer for regularization\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         # Register a buffer for the causal mask\n",
    "#         self.register_buffer(\n",
    "#             'mask',\n",
    "#             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "#         )\n",
    "\n",
    "#         # Feedforward method to compute the output of the multi-head attention\n",
    "#         def forward(self, x):\n",
    "#             # b: batch size\n",
    "#             # num_tokens: number of tokens in the sequence\n",
    "#             # d_in: input dimension\n",
    "#             b, num_tokens, d_in = x.shape\n",
    "\n",
    "#             # Compute Q, K, V\n",
    "#             # No need for explicit matrix multiplication, nn.Linear handles it internally\n",
    "#             keys = self.W_key(x)\n",
    "#             queries = self.W_query(x)\n",
    "#             values = self.W_value(x)\n",
    "\n",
    "#             # Reshape and transpose for multi-head attention\n",
    "#             keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#             queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#             values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose\n",
    "\n",
    "#             # Calculating attention scores\n",
    "#             attn_scores = queries @ keys.transpose(2, 3)\n",
    "#             # Make the mask boolean for current number of tokens \n",
    "#             mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "#             # Attention scores masking\n",
    "#             attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "#             # Turning scores into weights\n",
    "#             attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "#             # Apply dropout to attention weights\n",
    "#             attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\n",
    "#             # Construct context vectors\n",
    "#             context_vecs = attn_weights @ values\n",
    "\n",
    "#             context_vecs = context_vecs.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "#             output = self.out_proj(context_vecs)\n",
    "#             return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36200df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2403, -0.5516, -0.5697],\n",
      "        [ 1.0076, -0.0770, -1.0205],\n",
      "        [-0.1690,  0.9178, -0.3885],\n",
      "        [-0.9343, -0.4991, -1.0867]])\n",
      "\n",
      "\n",
      "tensor([[ 0.9624,  0.2492, -0.4845],\n",
      "        [-2.0929,  0.0983, -0.0935],\n",
      "        [ 0.2662, -0.5850, -0.3430],\n",
      "        [-0.6821, -0.9887, -1.7018]])\n",
      "\n",
      "\n",
      "tensor([[[ 0.2403, -0.5516, -0.5697],\n",
      "         [ 1.0076, -0.0770, -1.0205],\n",
      "         [-0.1690,  0.9178, -0.3885],\n",
      "         [-0.9343, -0.4991, -1.0867]],\n",
      "\n",
      "        [[ 0.9624,  0.2492, -0.4845],\n",
      "         [-2.0929,  0.0983, -0.0935],\n",
      "         [ 0.2662, -0.5850, -0.3430],\n",
      "         [-0.6821, -0.9887, -1.7018]]])\n",
      "b: 2\n",
      "num_tokens: 4\n",
      "d_in: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2,4,3)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "print(x[0])\n",
    "print(\"\\n\")\n",
    "print(x[1])\n",
    "print(\"\\n\")\n",
    "print(x)\n",
    "\n",
    "b, num_tokens, d_in = x.shape\n",
    "print(\"b:\", b)\n",
    "print(\"num_tokens:\", num_tokens)\n",
    "print(\"d_in:\", d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b57be86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5144,  0.3530,  0.2049],\n",
      "        [ 0.1255,  0.1361,  0.2230],\n",
      "        [-0.0746, -0.5366, -0.3570],\n",
      "        [ 0.4928,  0.0345, -0.4677],\n",
      "        [ 0.0911,  0.4770, -0.5456],\n",
      "        [-0.3887, -0.2299,  0.0232],\n",
      "        [-0.1347, -0.0634, -0.5629],\n",
      "        [ 0.2704,  0.5068,  0.3529]], requires_grad=True)\n",
      "\n",
      "\n",
      "tensor([[[-0.5967, -0.6374,  0.7212,  0.3788,  0.3062, -0.5439,  0.2889,\n",
      "          -0.0084],\n",
      "         [-0.1268, -0.5771,  0.5702,  0.9841,  0.8485, -0.9617,  0.4092,\n",
      "           0.2804],\n",
      "         [-0.2514, -0.4483, -0.1014,  0.1431,  0.8711, -0.7185,  0.1488,\n",
      "           0.6895],\n",
      "         [-1.2884, -0.8930,  0.9653,  0.0435,  0.5063, -0.1114,  0.7347,\n",
      "          -0.4819]],\n",
      "\n",
      "        [[ 0.0749, -0.4188,  0.2072,  0.7225,  0.7075, -1.0068,  0.0929,\n",
      "           0.6227],\n",
      "         [-1.4700, -0.7355,  0.3765, -0.9713,  0.1440,  0.2246,  0.2938,\n",
      "          -0.1419],\n",
      "         [-0.5487, -0.5882,  0.6563,  0.2844,  0.1690, -0.5411,  0.1599,\n",
      "           0.0616],\n",
      "         [-1.4576, -1.0652,  1.4288,  0.4386,  0.6314, -0.1111,  1.0780,\n",
      "          -0.8789]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "W_query = nn.Linear(3,8)\n",
    "print(W_query.weight)\n",
    "print(\"\\n\")\n",
    "queries = W_query(x)\n",
    "print(queries)\n",
    "print(queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15440c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5967, -0.6374,  0.7212,  0.3788],\n",
      "          [ 0.3062, -0.5439,  0.2889, -0.0084]],\n",
      "\n",
      "         [[-0.1268, -0.5771,  0.5702,  0.9841],\n",
      "          [ 0.8485, -0.9617,  0.4092,  0.2804]],\n",
      "\n",
      "         [[-0.2514, -0.4483, -0.1014,  0.1431],\n",
      "          [ 0.8711, -0.7185,  0.1488,  0.6895]],\n",
      "\n",
      "         [[-1.2884, -0.8930,  0.9653,  0.0435],\n",
      "          [ 0.5063, -0.1114,  0.7347, -0.4819]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0749, -0.4188,  0.2072,  0.7225],\n",
      "          [ 0.7075, -1.0068,  0.0929,  0.6227]],\n",
      "\n",
      "         [[-1.4700, -0.7355,  0.3765, -0.9713],\n",
      "          [ 0.1440,  0.2246,  0.2938, -0.1419]],\n",
      "\n",
      "         [[-0.5487, -0.5882,  0.6563,  0.2844],\n",
      "          [ 0.1690, -0.5411,  0.1599,  0.0616]],\n",
      "\n",
      "         [[-1.4576, -1.0652,  1.4288,  0.4386],\n",
      "          [ 0.6314, -0.1111,  1.0780, -0.8789]]]], grad_fn=<ViewBackward0>)\n",
      "After transpose\n",
      "\n",
      "tensor([[[[-0.5967, -0.6374,  0.7212,  0.3788],\n",
      "          [-0.1268, -0.5771,  0.5702,  0.9841],\n",
      "          [-0.2514, -0.4483, -0.1014,  0.1431],\n",
      "          [-1.2884, -0.8930,  0.9653,  0.0435]],\n",
      "\n",
      "         [[ 0.3062, -0.5439,  0.2889, -0.0084],\n",
      "          [ 0.8485, -0.9617,  0.4092,  0.2804],\n",
      "          [ 0.8711, -0.7185,  0.1488,  0.6895],\n",
      "          [ 0.5063, -0.1114,  0.7347, -0.4819]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0749, -0.4188,  0.2072,  0.7225],\n",
      "          [-1.4700, -0.7355,  0.3765, -0.9713],\n",
      "          [-0.5487, -0.5882,  0.6563,  0.2844],\n",
      "          [-1.4576, -1.0652,  1.4288,  0.4386]],\n",
      "\n",
      "         [[ 0.7075, -1.0068,  0.0929,  0.6227],\n",
      "          [ 0.1440,  0.2246,  0.2938, -0.1419],\n",
      "          [ 0.1690, -0.5411,  0.1599,  0.0616],\n",
      "          [ 0.6314, -0.1111,  1.0780, -0.8789]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# .view(b,num_tokens,num_heads, head_dim)\n",
    "# head_dim: number of features per head\n",
    "# num_heads: number of parallel attention heads\n",
    "# b: batch size\n",
    "# num_tokens: number of tokens per batch\n",
    "queries_reshaped = queries.view(2,4,2,4)\n",
    "print(queries_reshaped)\n",
    "\n",
    "print(\"After transpose\\n\")\n",
    "\n",
    "print(queries_reshaped.transpose(1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
