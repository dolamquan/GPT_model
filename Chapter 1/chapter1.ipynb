{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07173d7c",
   "metadata": {},
   "source": [
    "## Chapter 1: Understanding LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc145fd",
   "metadata": {},
   "source": [
    "**Stages of building and using LLMs**\n",
    "- LLM is pretrained on unlabeled text data\n",
    "- The LLM has a few basic capabilities after pretraining\n",
    "- A pretrained LLM can be further trained on a labeled dataset to obtain a fine-tuned LLM for specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ad175",
   "metadata": {},
   "source": [
    "**Pretraining**\n",
    "- Creating an initial pretrained LLM, often called a base or foundation model. Ex: GPT-3 model\n",
    "- After obtaining a pretrained LLM from training on large text datasets, where LLM is trained to predict the next word in the text, we can further train the LLM on labeled data - fine tuning\n",
    "\n",
    "**Fine-tuning**\n",
    "- Instruction fine-tuning: Labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text\n",
    "- Classification fine-tuning: Labeled dataset consists of texts and associated class labels - for example, emails associated with \"spam\" and \"not spam\" labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af27822",
   "metadata": {},
   "source": [
    "**Introduction to Transformer Architecture**\n",
    "- Most modern LLMs rely on the transformer architecture\n",
    "\n",
    "![Transformer Architecture](pic1.png)\n",
    "\n",
    "- The transformer architecture consists of two submodules: an encoder and a decoder\n",
    "\n",
    "**Encoder:**\n",
    "- Processes the input text and encodes it into a series of numerical representations of vectors that cupture the contextual information of the input\n",
    "\n",
    "**Decoder:**\n",
    "- Takes the encoded vectors and generates the output text\n",
    "\n",
    "**Self-attention Mechanism:**\n",
    "- Enables the model to capture long-ranged dependencies and contextual relationships within the input data --> enhancing its ability to generate coherent and contextually relevant output\n",
    "\n",
    "![BERT VS GPT models](pic2.png)\n",
    "\n",
    "**BERT:**\n",
    "- Built upon the original transformer's encoder submodule, differes in its training approach from GPT\n",
    "- Specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence.\n",
    "- This unique training strategy equips BERT with strengths in text classification tasks, including sentiment\n",
    "\n",
    "**GPT:**\n",
    "- GPT focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts\n",
    "- This includes machine translation, text summarization, fiction writing, writing computer code and more.\n",
    "- GPT models primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb788d",
   "metadata": {},
   "source": [
    "### Closer Look at the GPT architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c66ff",
   "metadata": {},
   "source": [
    "- GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset\n",
    "- In addition, the original model offered in ChatGPT was created by fine-tuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper\n",
    "- The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling\n",
    "- This means that we don't need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict.\n",
    "- Since this next-word prediction task alows us to create labels \"on the fly,\" it is possible to use massive unlabeled text datasets to train ML\n",
    "- The general GPT architecture is relatively simple\n",
    "- Essentially, it is just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of **autoregressive** model\n",
    "- **Autoregressive Model:** incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e76c7",
   "metadata": {},
   "source": [
    "### Building a Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285e279",
   "metadata": {},
   "source": [
    "![Building a LLM](pic3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
