{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46628067",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Source: StatQuest with Josh Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac903e",
   "metadata": {},
   "source": [
    "### I. Introduction to Neural Networks\n",
    "\n",
    "Link: \n",
    "\n",
    "A Neural Network consists of Nodes  and connections between the nodes. The numbers along each connection represent parameter values that were estimated when this Neural Network was fit to the data.\n",
    "\n",
    "A Neural Network starts out with unknown parameter values that are estimated when we fit the Neural Network to a dataset using a method called **Backpropagation**\n",
    "\n",
    "**Description of Neural Network:**\n",
    "\n",
    "*Assume that we have already estimated the parameters through backpropagation*\n",
    "\n",
    "Some of the Nodes have **curved lines** inside of them. These lines are the building blocks for fitting a function into the data. These curves can be reshaped by modifying the parameters and then added together to form the overall function\n",
    "\n",
    "The specific curved line is called softplus, alternatively, we can also use bent line, which is called **ReLU** or we can use a **sigmoid** shape or any other bent or curved line. These lines are called **Activation Functions.**\n",
    "\n",
    "When you build a Neural Network, you have to decide which Activation Function or Functions you want to use.\n",
    "\n",
    "When most people teach Neural Networks, they use the sigmoid Activation Function; however, in reality, ReLU Activation or softplus is more common.\n",
    "\n",
    "The layer of nodes between the **Input** and **Ouput** nodes are called **Hidden Layers.**\n",
    "\n",
    "When you build a neural network, one of the first things you do is to decide how many Hidden Layers you want, and how many nodes go into each hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015e41c",
   "metadata": {},
   "source": [
    "## II. Backpropagation\n",
    "\n",
    "Link1: https://www.youtube.com/watch?v=IN2XmBhILt4\n",
    "Link2: https://www.youtube.com/watch?v=iyn2zdALii8\n",
    "\n",
    "**Main Ideas:**\n",
    "- Using Chain Rule to calculate derivatives\n",
    "- Plugging the derivatives in Gradient Descent to optimize parameters\n",
    "\n",
    "First of all, when we start setting up the neural networks, we have to assign random weights and biases to each node and then we would use backpropagation to optimize these values starting from the outmost layer.\n",
    "\n",
    "We can quantify how good the function fits the data by calculating the **Sum of the Squared Residuals (SSR).** A residual is the difference between the **Observed** and **Predicted** values\n",
    "\n",
    "However, instead of plugging in tons of values to find the lowest point in the curve (bias vs SSR), we can use Gradient Descent to find it relatively quickly. That means that we need to find the derivative of the SSR with respect to the bias. We can plug this derivative into Gradient Descent to find the optimal value for bias \n",
    "\n",
    "\n",
    "**Second Part:**\n",
    "*Assume that we don't know the optimized parameters for w3, w4, and b3 and we know that for w1,w2,b1, and b2.*\n",
    "\n",
    "We first derive the derivative of SSR with respect to w3, w4, and b3, and we plug them into gradient descent to optimize their values.\n",
    "\n",
    "**Third Part:**\n",
    "*Assume that we have to optimize all of the parameters from the start*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
