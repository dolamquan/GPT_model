{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49098852",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eb403",
   "metadata": {},
   "source": [
    "### Understanding Word Embeddings\n",
    "\n",
    "- Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn't compatible with the mathematical operations used to implement and train neural networks\n",
    "- Therefore, we need a way to represent words as continuous-valued vectors\n",
    "- **Embedding:** Converting data into a vector format\n",
    "- At its core, embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space - the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process\n",
    "- While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents.\n",
    "- Sentence or paragraph embeddings are popular choices for *retrieval-augmented* generation\n",
    "- Retrieval-augmented generation combines generation with retrieval to pull relevant information when generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab0228",
   "metadata": {},
   "source": [
    "- Several algorithms and frameworks have been developed to generate word embeddings.\n",
    "- One of the earlier and most popular examples is the *Word2Vec* approach.\n",
    "- *Word2Vec:* trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa.\n",
    "- LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4479f38",
   "metadata": {},
   "source": [
    "### Tokenizing Text\n",
    "\n",
    "- Split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM\n",
    "- These tokens are either individual words or special characters, including punctuation characters.\n",
    "\n",
    "![Tokenizing Text](pic4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b91970c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Tokenize practice on a sample text\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99]) \n",
    "\n",
    "# Our goal is to tokenize this 20,479-character short story into individual words and special characters that we can then turn into \n",
    "# embeddings for training a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e96e32f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'my', ' ', 'name', ' ', 'is', ' ', 'Jack.Doe!', ' ', \"How's\", ' ', 'it', ' ', 'going?']\n",
      "['Hello', ',', '', ' ', 'my', ' ', 'name', ' ', 'is', ' ', 'Jack', '.', 'Doe!', ' ', \"How's\", ' ', 'it', ' ', 'going?']\n",
      "['Hello', ',', 'my', 'name', 'is', 'Jack', '.', 'Doe!', \"How's\", 'it', 'going?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Use re.split command to split a text on whitespace characters\n",
    "text = \"Hello, my name is Jack.Doe! How's it going?\"\n",
    "tokens =re.split(r'(\\s)', text)\n",
    "print(tokens)\n",
    "\n",
    "# Split on whitespaces (\\s), commas, and periods([,.])\n",
    "tokens = re.split(r'(\\s|[,.])',text)\n",
    "print(tokens)\n",
    "\n",
    "# Remove redundant characters\n",
    "tokens = [item for item in tokens if item.strip()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51542967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed_tokens = [item for item in preprocessed_tokens if item.strip()]\n",
    "print(len(preprocessed_tokens))\n",
    "print(preprocessed_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ae16e",
   "metadata": {},
   "source": [
    "### Converting Tokens into token IDs\n",
    "- Convert these tokens from a Python string to an integer representation to produce the token IDs\n",
    "- This conversion is an intermediate step before converting the token IDs into embedding vectors\n",
    "- To map the previously generated tokesn in token IDs, we have to build a vocabulary that map each unique word and special character\n",
    "\n",
    "![Build Vocabulary By Tokening](pic5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8512b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1159\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n"
     ]
    }
   ],
   "source": [
    "allwords = sorted(set(preprocessed_tokens))\n",
    "vocab_size = len(allwords)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Create a vocabulary that maps each unique word and special character to a unique integer\n",
    "vocab = {token: integer for integer, token in enumerate(allwords)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "# Dictionary contains individual tokens associated with unique integer lables\n",
    "# --> Our next goal is to apply this vocabulary to convert new text into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a590106",
   "metadata": {},
   "source": [
    "![Tokenizing new text](pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98612fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we want to convert the outputs of an LLM from numbers back into text\n",
    "# We need a way to turn token IDs into text --> create an inverse version of the vocabulary that \n",
    "# maps token IDs back to the corresponding text tokens\n",
    "\n",
    "\n",
    "# Class tokenizer: with encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs\n",
    "# We will also implement decode to reverse integer-to-string mapping to convert the token IDs back into text\n",
    "\n",
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # Dictionary mapping tokens to unique integer labels\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    # Splits text into tokens and carries out string-to-integer mapping to produce token IDs\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    # Carries out reverse integer-to-string mapping to convert the token IDs back into text\n",
    "    def decode(self,token_ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in token_ids])\n",
    "        text = re.sub(r'\\s+([,\\.?\\!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677795c",
   "metadata": {},
   "source": [
    "![Encode and Decode Pipeline](pic7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "97482595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "Decoded Text: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "sample_text = \"\"\"\"It's the last he painted, you know,\"\n",
    "                  Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "\n",
    "# Error will occur if we try to encode a text that contains a token not in the vocabulary\n",
    "# This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a80eab",
   "metadata": {},
   "source": [
    "## Adding Special Context Tokens\n",
    "- We need to modify the tokenizer to handle unknown words\n",
    "- We also need to address the usage and addition of sepcial context tokens that can enhance a model's understanding of cotext or other relevant information in the text\n",
    "- These special tokens can include markers for unknwon words and document boundaries. \n",
    "- Create a new tokenizer to support two new tokens ,<|unk|> and <|endoftext|>\n",
    "\n",
    "\n",
    "![Taking into account unknown tokens](pic8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9162120",
   "metadata": {},
   "source": [
    "- We can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "- Furthermore, we add a token between unrelated texts\n",
    "- Ex: When training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source --> helps LLM understand that these text sources are concatenated for training, they are unrelated\n",
    "\n",
    "![Separating Documents](pic9.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37056119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n"
     ]
    }
   ],
   "source": [
    "# Modify the vocabulary to include special tokens for unknown words and end of text\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed_tokens))) # Take the list of tokens and removes duplicates then converts to list and sort it --> produce a unique, ordered list of tokens\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"]) # Appends two special tokens to the end of the list\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)} # Recreate the vocabulary dictionary mapping each token to a unique integer label\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e8e51e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "for item in list(vocab.items())[-5:]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aea1d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # Dictionary mapping tokens to unique integer labels\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    # Splits text into tokens and carries out string-to-integer mapping to produce token IDs\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    # Carries out reverse integer-to-string mapping to convert the token IDs back into text\n",
    "    def decode(self,token_ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in token_ids])\n",
    "        text = re.sub(r'\\s+([,\\.?\\!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a9085bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs with unknown token: [101, 595, 119, 1160, 1160, 1136, 1160, 1160, 7]\n",
      "Decoded Text with unknown token: This is a <|unk|> <|unk|> with <|unk|> <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text_with_unknown = \"\"\"This is a new sentence with unkwn words.\"\"\"\n",
    "tokenizer_v2 = TokenizerV2(vocab)\n",
    "token_ids_v2 = tokenizer_v2.encode(text_with_unknown)\n",
    "print(\"Token IDs with unknown token:\", token_ids_v2)\n",
    "decoded_text_v2 = tokenizer_v2.decode(token_ids_v2)\n",
    "print(\"Decoded Text with unknown token:\", decoded_text_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fec53",
   "metadata": {},
   "source": [
    "- Depending on the LLM, some researches also consider additional special tokens such as the following:\n",
    "\n",
    "    - [BOS]: Beginning of sequence - this token marks the start of a text. It signifies to the LLM where a piece of content begins\n",
    "    - [EOS]: End of sequence - this token positioned at the end of a text and is expecially useful when concatenating multiple unrelated texts\n",
    "    - [PAD]: Padding - when training LLMs with batch sizes larger the one, the bath might contrain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using [PAD] token, up to the lenght of the longes text in the batch\n",
    "\n",
    "- Tokenizer used for GPT models does not need any of these tokens; it only uses <|endoftext|> token for simplicity\n",
    "- Tokenizer used for GPT models doesn't use an <|unk|> token for out-of-vocabulary words. Instead GPT models use **by pair encoding tokenize**, which breaks words down into subword units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a56247",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "- BPE was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7c122da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jackd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7c6f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "# tiktoken is a fast BPE tokenizer used by OpenAI for GPT models\n",
    "\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "603f63ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs using tiktoken: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "         \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integer_ids = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(\"Token IDs using tiktoken:\", integer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9adfe398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text using tiktoken: Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integer_ids)\n",
    "print(\"Decoded text using tiktoken:\", strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cad5f5",
   "metadata": {},
   "source": [
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocuabulary into smaller subword units or even individual charactacters, enabling it to handle out-of-vocabulary words. With the support of BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters\n",
    "\n",
    "![Tokenizing Unknown Words](pic10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e311d7",
   "metadata": {},
   "source": [
    "## Data Sampling with a Sliding Window\n",
    "- The next step in creating the embeddings for the LLM is to generate the input-target pairs required for training an LLM\n",
    "- LLMs are pretrained by predicting the next word in a text\n",
    "\n",
    "![Data Sampling With a Sliding Window](pic11.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3769bc7",
   "metadata": {},
   "source": [
    "Let's implement a data loader that fetches the input-target pairs in figure 2.12 from the training dataset using a sliding window approach. To get started, we will tokenize the whole \"The Verdict\" short story using the BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ef8ab25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1fd90236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# Remove the first 50 tokens from the dataset for demonstration purposes\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# x: contains the inputs token\n",
    "# y: contains the target tokens (i.e., the next token to predict)\n",
    "\n",
    "context_size = 4 # number of tokens in the input sequence\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ac47536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "# By processing the inputs along with the targets, which are the inputs shifted by one position, we can create\n",
    "# the next-word prediction tasks\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context,\"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87e14838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# Everything left of the arrow refers to the input an LLM would receive\n",
    "# The token ID on the right side of the arrow represents that target token ID that the LLM is supposed to predict\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e6d8e",
   "metadata": {},
   "source": [
    "We have now created the input-target pairs that we can use for LLM training --> There is only one more task before we can turn the tokens into embeddings: implementing efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
    "\n",
    "In particular, we are interested in returning two sensors: input sensor containing the text that the LLM sees and a target sensor that includes the targets for the LLM to predict\n",
    "\n",
    "![Implement efficient data loader implementation](pic12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d8ec6",
   "metadata": {},
   "source": [
    "For the efficient data loader implementation, we will use PyTorch's built-int *Dataset* and *DataLoader* classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18640b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp311-cp311-win_amd64.whl (109.3 MB)\n",
      "     -------------------------------------- 109.3/109.3 MB 7.7 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jackd\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 8.7 MB/s eta 0:00:00\n",
      "Collecting networkx>=2.5.1\n",
      "  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 9.3 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.9/134.9 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting fsspec>=0.8.5\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "     ------------------------------------- 199.3/199.3 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ------------------------------------- 536.2/536.2 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "24d4386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Take a long text string, tokenize it, and split it into sliding windows of training samples - each window\n",
    "# being a sequence of tokens of fixed max_length\n",
    "# Each sample teaches the model to predict the next token in a sequence\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        tokens_ids = tokenizer.encode(txt) # Tokenizes the entire text\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length,stride): # Uses a sliding window to chunk the book into overlapping sequences of max_length\n",
    "            input_chunk = tokens_ids[i:i+max_length] \n",
    "            target_chunk = tokens_ids[i+1:i+max_length+1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self): # Returns the total number of rows in the datasedt\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # Returns a single row from the dataset\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5512a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function wraps the dataset inside a PyTorch DataLoader, which handles batching and shuffling\n",
    "# It is to prepare batches of (input,target) pairs efficiently for model training\n",
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "96ccced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text,batch_size=1,max_length=4,stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader) # Converts dataloader into a Python iterator to fetch the next entry via Python's built in next() function\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556591a2",
   "metadata": {},
   "source": [
    "![Meaning of stride](pic13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247eaf0b",
   "metadata": {},
   "source": [
    "## Creating Token Embeddings\n",
    "- The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors\n",
    "- As a preliminary step, we must initialize these embedding weights with random values\n",
    "- This initialization serves as the starting point for the LLM's learning process\n",
    "- **Embedding:** Is a way to represent something as a vector of numbers, usually in a high-dimensional space\n",
    "\n",
    "![Creating Token Embeddings](pic14.png)\n",
    "\n",
    "A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "32fadcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2,3,4,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(embedding_layer.weight)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9257f7",
   "metadata": {},
   "source": [
    "The weight matrix of the embedding layer contains small, random values. These values are then optimized during LLM training as part of the LLM optimization itself.\n",
    "\n",
    "Moreover, we can see that the weight matrix has six rows and three columns. There is one row fo each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "286eada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "34765e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb7e32",
   "metadata": {},
   "source": [
    "![Weight Matrix of Embedding Layer](pic15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bce791",
   "metadata": {},
   "source": [
    "## Encoding Word Positions\n",
    "- Each otken ID gets turned into a vector using the embedding matrix\n",
    "    - Every word has its own unique vector of numbers\n",
    "    - But- - the same word always gets the same vector, no matter where it appears in the sentence\n",
    "\n",
    "**Problem:**\n",
    "- The model can't tell which word came first and which came last.\n",
    "- This matters because language depends on word order, but because embeddings don't include position, the model has no built-in sense of sequence or structure\n",
    "\n",
    "![Positional Econdings](pic16.png)\n",
    "\n",
    "**Solution:**\n",
    "- Add positional encodings (extra vectors) to each token embeddings to tell the model where each token is in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "badc8b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m dataloader = create_dataloader_v1(raw_text,batch_size=\u001b[32m8\u001b[39m,max_length=max_length,stride=max_length, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m data_iter = \u001b[38;5;28miter\u001b[39m(dataloader)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m inputs, targets = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mToken IDs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, inputs)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInputs shape:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m,inputs.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jackd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jackd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:787\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    788\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jackd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:722\u001b[39m, in \u001b[36m_BaseDataLoaderIter._next_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text,batch_size=8,max_length=max_length,stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\\\n\",inputs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
