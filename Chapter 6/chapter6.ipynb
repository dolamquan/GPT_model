{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee3cf9a",
   "metadata": {},
   "source": [
    "# Fine-Tuning For Classification\n",
    "- Introduction to training model for doing different kind of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f430b5a",
   "metadata": {},
   "source": [
    "The most common ways to fine-tune language models are instruction fine-tuning and classification fine-tuning:\n",
    "\n",
    "**Instruction Fine-Tuning:**\n",
    "- Training a language model on a set of tasks using specific instructions to improve its ability to understand and execute tasks described in natural language prompts\n",
    "\n",
    "**Classification Fine-Tuning:**\n",
    "- The model is trained to recognize a specific set of class labels, such as \"spam\" and \"not spam\"\n",
    "- Examples of classification tasks extend beyond LLMs and email filtering: they include identifying different species of plants from images; categorizing news articles into topics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54694766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file extracted to sms_spam_collection\\SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "# Prepare Dataset for Classification Fine-Tuning\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(\"Data file already exists. Skipping download.\")\n",
    "        return\n",
    "    \n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, 'wb') as out_file:\n",
    "            out_file.write(response.read())\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"Data file extracted to {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d7d10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=['Label', 'Text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87402691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     747\n",
       "spam    747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a balanced dataset\n",
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df['Label'] == 'spam'].shape[0]\n",
    "    ham_samples = df[df['Label'] == 'ham'].sample(n=num_spam, random_state=123)\n",
    "\n",
    "    balanced_df = pd.concat([ham_samples, df[df['Label'] == 'spam']])\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff07244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "balanced_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9edbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 3 parts: train, validation, test\n",
    "# 70% train, 10% validation, 20% test\n",
    "\n",
    "def random_split(df, train_frac,validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)  # Shuffle the dataframe\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    val_end = int(len(df) * (train_frac + validation_frac))\n",
    "\n",
    "    train_df = df.iloc[:train_end]\n",
    "    validation_df = df.iloc[train_end:val_end]\n",
    "    test_df = df.iloc[val_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cce8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "validation_df.to_csv(\"validation.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70d70bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Creating data loaders\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Build PyTorch DataLoaders for variable-length textmessages (spam dataset): Since messages have different lengths, batching requires\n",
    "all sequences in a batch to have th same length -> 2 ways to achieve this:\n",
    "\n",
    "1. Truncate to the shortest message:\n",
    "- Simple but wastes data\n",
    "- Worse model performance\n",
    "\n",
    "2. Pad to the longest message\n",
    "- Preserves all data\n",
    "- Slightly more complex implementation but better model performance\n",
    "- Chosen as the better option \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\",allowed_special={\"<|endoftext|>\"}))  # gpt2 endoftext token ID is 50256\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None,pad_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}) for text in self.data['Text']\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encoded = self.encoded_texts[idx]\n",
    "        label = self.data.iloc[idx]['Label']\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_len = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            if len(encoded_text) > max_len:\n",
    "                max_len = len(encoded_text)\n",
    "        return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43dcadb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset max length: 120\n"
     ]
    }
   ],
   "source": [
    "# Preparing Training, Validation, and Test Datasets\n",
    "\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file = \"train.csv\",\n",
    "    max_length = None,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "print(f\"Training dataset max length: {train_dataset.max_length}\")\n",
    "\n",
    "validation_dataset = SpamDataset(\n",
    "    csv_file = \"validation.csv\",\n",
    "    max_length = train_dataset.max_length,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file = \"test.csv\",\n",
    "    max_length = train_dataset.max_length,\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99505bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "Once the SpamDataset objects are created, we can build PyTorch DataLoaders\n",
    "DataLoaders are used for:\n",
    "- Batching the data:\n",
    "- Shuffling the data\n",
    "- Automatic padding - even though we have already padded the sequences to the max length in the dataset, DataLoader can handle this as well\n",
    "- Converts Python Data -> PyTorch Tensors\n",
    "\"\"\"\n",
    "\n",
    "num_workers = 0  # Number of subprocesses to use for data loading\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    dataset = validation_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = num_workers,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = num_workers,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e0d3f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([8, 120])\n",
      "Target batch shape: torch.Size([8])\n",
      "130 training samples batches\n",
      "19 validation samples batches\n",
      "38 test samples batches\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "print(\"Target batch shape:\", target_batch.shape)\n",
    "\n",
    "print(f\"{len(train_loader)} training samples batches\")\n",
    "print(f\"{len(validation_loader)} validation samples batches\")\n",
    "print(f\"{len(test_loader)} test samples batches\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bf580d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initalizing Model with pretrained weights\n",
    "from chapter4 import GPTModel\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"  # Options: \"gpt2-small (124M)\", \"gpt2-medium (355M)\", \"gpt2-large (774M)\", \"gpt2-xl (1558M)\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 256,\n",
    "    'embed_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'dropout': 0.1,\n",
    "    'qkv_bias': False,\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"embed_dim\": 768, \"n_layers\":12, \"n_heads\":12},\n",
    "    \"gpt2-medium (355M)\": {\"embed_dim\": 1024, \"n_layers\":24, \"n_heads\":16},\n",
    "    \"gpt2-large (774M)\": {\"embed_dim\": 1280, \"n_layers\":36, \"n_heads\":20},\n",
    "    \"gpt2-xl (1558M)\": {\"embed_dim\": 1600, \"n_layers\":48, \"n_heads\":25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "BASE_CONFIG.update({\"context_length\":1024}) \n",
    "BASE_CONFIG.update({\"qkv_bias\":True}) \n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer_v3.pth\", map_location=torch.device('cpu'))\n",
    "model = GPTModel(BASE_CONFIG) # Initialize model\n",
    "model.load_state_dict(checkpoint['model_state_dict']) # Load pretrained weights\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # Initialize optimizer\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # Load optimizer statement\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fa7eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Every effort moves you with the torch, and grows your spirit.\n",
      "\n",
      "KEEP BREAK\n"
     ]
    }
   ],
   "source": [
    "from chapter4 import generate_text\n",
    "from chapter5 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens = 15,\n",
    "    context_size = BASE_CONFIG['context_length'],\n",
    ")\n",
    "print(\"Generated text:\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c381fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention_v2(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a Classifcation Head for the Model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze the pretrained model parameters\n",
    "\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "num_classes = 2  # Spam and Ham\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features = BASE_CONFIG['embed_dim'],\n",
    "    out_features = num_classes\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c043ab",
   "metadata": {},
   "source": [
    "**Fine-tuning Selected Layers VS. All Layers**\n",
    "- Since we start with a pretrained model, it is not necessary to fine-tune all model layers\n",
    "- In neural network-based language models, the lower layers generally capture basic language structures and semantics applicable across a wide range of tasks and datasets -> fine-tuning only the last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "818f574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final LayerNorm and last transformer block trainable\n",
    "# .requires_grad: Controls whether a parameter will be updated during backpropagation\n",
    "\n",
    "\"\"\"\n",
    "Make only the last transformer block and final LayerNorm trainable -> everything else is frozen\n",
    "- Wants the model to output a class label, not a next token prediction\n",
    "\n",
    "Only use the last token's representation for classification:\n",
    "- This is because of the casual attention mask (GPT-style attention):\n",
    "- Each token can only attend to previous tokens and itself\n",
    "- The last token's representation contains information from all previous tokens in the sequence\n",
    "- Using only the last token's representation simplifies the classification task\n",
    "- Allows the model to focus on learning the mapping from the final representation to the class labels\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990dcab2",
   "metadata": {},
   "source": [
    "## Calculating the Classification Loss and Accuracy\n",
    "\n",
    "- Before implementing the evalation utilities, let's briefly discuss how we convert the model outputs into class label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582807e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "When doing classification, we replace the output layer so that:\n",
    "+ Output_dim = 2 instead of vocab_size\n",
    "+ Where the 2 output dimensions correspond to the 2 classes: spam and ham\n",
    "\n",
    "And we do the following process:\n",
    "1. Softmax over the 2 logits -> converts logits to probabilities for each class\n",
    "2. Argmax -> returns the class with the highest probability as the predicted class label\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use this to print out the accuracy of the model before fine-tuning\n",
    "# Not useful for training since this function is not differentiable\n",
    "def calc_accuracy_loader(data_loader,model, device, num_batches = None):\n",
    "    \"\"\"\n",
    "    Helps compute the classification acccracy of the fine-tuned GPT model on a dataset\n",
    "\n",
    "    Process:\n",
    "    1. It takes a DataLoader, model, device, and optional num_batches as input\n",
    "    2. It runs the model on those batches without training, just making predictions\n",
    "    3. It compares the model's predicted class labels to the true labels to compute accuracy\n",
    "    4. Counts how many predictions were correct and divides by total predictions to get accuracy\n",
    "\n",
    "    Implementation Details:\n",
    "    1. Switch model to eval mode with model.eval() to disable dropout and other training-specific layers\n",
    "    2. Initialize counters\n",
    "    3. Decide how many batches to evaluate\n",
    "    4. Loop over batches from the DataLoader\n",
    "    5. Run the model in no-gradient mode to save memory and computation\n",
    "    6. Convert logits to predicted class labels using argmax\n",
    "    7. Update counters based on correct predictions\n",
    "    8. After looping, compute accuracy as correct predictions / total predictions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            logits = model(input_batch)[:, -1, :]  # Get logits for the last token\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct += (predictions == target_batch).sum().item()\n",
    "            total += target_batch.size(0)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecfe0bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy before fine-tuning: 45.00%\n",
      "Initial training accuracy before fine-tuning: 48.75%\n",
      "Initial test accuracy before fine-tuning: 47.50%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "initial_validation_accuracy = calc_accuracy_loader(\n",
    "    data_loader = validation_loader,\n",
    "    model = model,\n",
    "    device = device,\n",
    "    num_batches = 10\n",
    ")\n",
    "print(f\"Initial validation accuracy before fine-tuning: {initial_validation_accuracy*100:.2f}%\")\n",
    "\n",
    "initial_train_accuracy = calc_accuracy_loader(\n",
    "    data_loader = train_loader,\n",
    "    model = model,\n",
    "    device = device,\n",
    "    num_batches = 10\n",
    ")\n",
    "print(f\"Initial training accuracy before fine-tuning: {initial_train_accuracy*100:.2f}%\")\n",
    "\n",
    "initial_test_accuracy = calc_accuracy_loader(\n",
    "    data_loader = test_loader,\n",
    "    model = model,  \n",
    "    device = device,\n",
    "    num_batches = 10\n",
    ")\n",
    "print(f\"Initial test accuracy before fine-tuning: {initial_test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e9e3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    Calculate the classification loss for a batch of input data and target labels using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - input_batch (torch.Tensor): A batch of input data (token IDs).\n",
    "    - target_batch (torch.Tensor): A batch of target class labels.\n",
    "    - model (torch.nn.Module): The model used for making predictions.\n",
    "    - device (torch.device): The device to run the computations on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "    - loss (torch.Tensor): The computed classification loss for the batch.\n",
    "    \"\"\"\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)[:, -1, :]  # Get logits for the last token\n",
    "    loss = nn.CrossEntropyLoss()(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec901b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    Calculate the average loss of the model on a dataset provided by data_loader.\n",
    "\n",
    "    Parameters:\n",
    "    - data_loader: DataLoader providing batches of input-target pairs.\n",
    "    - model: The neural network model to evaluate.\n",
    "    - loss_fn: Loss function to compute the loss.\n",
    "    - device: Device to run the computations on (CPU or GPU).\n",
    "    - num_batches: Optional number of batches to evaluate. If None, evaluates all batches.\n",
    "\n",
    "    Returns:\n",
    "    - Average loss over the evaluated batches.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i>= num_batches:\n",
    "                break\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            logits = model(input_batch)[:, -1, :]  # Get logits for the last token\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            batch_size = target_batch.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d720ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training loss before fine-tuning: 2.5661\n",
      "Initial validation loss before fine-tuning: 2.8269\n",
      "Initial test loss before fine-tuning: 2.6884\n"
     ]
    }
   ],
   "source": [
    "# Prinit initial losses before fine-tuning\n",
    "initial_train_loss = calc_loss_loader(\n",
    "    data_loader = train_loader,    \n",
    "    model = model,\n",
    "    device = device,\n",
    "    num_batches = 10\n",
    ")\n",
    "print(f\"Initial training loss before fine-tuning: {initial_train_loss:.4f}\")\n",
    "\n",
    "initial_validation_loss = calc_loss_loader(\n",
    "    data_loader = validation_loader,\n",
    "    model = model,\n",
    "    device = device,\n",
    "    num_batches = 10\n",
    ")\n",
    "print(f\"Initial validation loss before fine-tuning: {initial_validation_loss:.4f}\")\n",
    "\n",
    "initial_test_loss = calc_loss_loader(\n",
    "    data_loader = test_loader,\n",
    "    model = model,\n",
    "    device = device,\n",
    "    num_batches = 10\n",
    ")\n",
    "print(f\"Initial test loss before fine-tuning: {initial_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa23fd",
   "metadata": {},
   "source": [
    "![](pic1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e5cb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Compute average loss on training and validation sets\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()  # Switch back to training mode\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8363403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the Model on supervised dataset\n",
    "\"\"\"\n",
    "Parameters:\n",
    "model: The neural network model to be trained.\n",
    "train_loader: DataLoader for the training dataset.\n",
    "validation_loader: DataLoader for the validation dataset.\n",
    "device: The device (CPU or GPU) to run the computations on.\n",
    "num_epochs: Number of training epochs.\n",
    "learning_rate: Learning rate for the optimizer.\n",
    "eval_freq: Frequency (in iterations) to evaluate the model on the validation set.\n",
    "eval_iter: Number of iterations to run evaluation.\n",
    "\n",
    "\"\"\"\n",
    "def train_model(model, train_loader, validation_loader, device, num_epochs,eval_freq, eval_iter,optimizer):\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    examples_seen = 0\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, validation_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                validation_losses.append(val_loss)\n",
    "\n",
    "                print(f\"Ep {epoch+1}, Step {global_step}, Examples Seen {examples_seen}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        train_acc = calc_accuracy_loader(train_loader, model, device)\n",
    "        val_acc = calc_accuracy_loader(validation_loader, model, device)\n",
    "\n",
    "        print(f\"Training Accuracy after Epoch {epoch+1}: {train_acc*100:.2f}%\")\n",
    "        print(f\"Validation Accuracy after Epoch {epoch+1}: {val_acc*100:.2f}%\")\n",
    "        train_accuracies.append(train_acc)\n",
    "        validation_accuracies.append(val_acc)\n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies, examples_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06f9b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1, Step 50, Examples Seen 400, Train Loss: 0.5430, Validation Loss: 0.7133\n",
      "Ep 1, Step 100, Examples Seen 800, Train Loss: 0.5841, Validation Loss: 0.6600\n",
      "Training Accuracy after Epoch 1: 81.54%\n",
      "Validation Accuracy after Epoch 1: 80.67%\n",
      "Ep 2, Step 150, Examples Seen 1200, Train Loss: 0.6914, Validation Loss: 0.7069\n",
      "Ep 2, Step 200, Examples Seen 1600, Train Loss: 0.4896, Validation Loss: 0.5340\n",
      "Ep 2, Step 250, Examples Seen 2000, Train Loss: 0.4009, Validation Loss: 0.4837\n",
      "Training Accuracy after Epoch 2: 74.13%\n",
      "Validation Accuracy after Epoch 2: 68.00%\n",
      "Ep 3, Step 300, Examples Seen 2400, Train Loss: 0.2879, Validation Loss: 0.5467\n",
      "Ep 3, Step 350, Examples Seen 2800, Train Loss: 0.4000, Validation Loss: 0.3797\n",
      "Training Accuracy after Epoch 3: 81.83%\n",
      "Validation Accuracy after Epoch 3: 79.33%\n",
      "Ep 4, Step 400, Examples Seen 3200, Train Loss: 0.3203, Validation Loss: 0.4229\n",
      "Ep 4, Step 450, Examples Seen 3600, Train Loss: 0.4745, Validation Loss: 0.4238\n",
      "Ep 4, Step 500, Examples Seen 4000, Train Loss: 0.3023, Validation Loss: 0.4871\n",
      "Training Accuracy after Epoch 4: 84.81%\n",
      "Validation Accuracy after Epoch 4: 84.67%\n",
      "Ep 5, Step 550, Examples Seen 4400, Train Loss: 0.5158, Validation Loss: 0.4476\n",
      "Ep 5, Step 600, Examples Seen 4800, Train Loss: 0.2329, Validation Loss: 0.3461\n",
      "Ep 5, Step 650, Examples Seen 5200, Train Loss: 0.5275, Validation Loss: 0.4502\n",
      "Training Accuracy after Epoch 5: 81.54%\n",
      "Validation Accuracy after Epoch 5: 78.67%\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 5e-5,\n",
    "    weight_decay = 0.1\n",
    ")\n",
    "train_losses, validation_losses, train_accuracies, validation_accuracies, examples_seen = train_model(\n",
    "    model = model, \n",
    "    train_loader = train_loader,\n",
    "    validation_loader = validation_loader,\n",
    "    device = device,\n",
    "    num_epochs = 5,\n",
    "    eval_freq = 50,\n",
    "    eval_iter = 5,\n",
    "    optimizer = optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28eb6a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_per_epoch(train_accs, val_accs):\n",
    "    \"\"\"\n",
    "    Plots how training and validation accuracy change over epochs.\n",
    "    - train_accs: list of training accuracy values per epoch\n",
    "    - val_accs:   list of validation accuracy values per epoch\n",
    "    \"\"\"\n",
    "\n",
    "    epochs = range(1, len(train_accs) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_accs, marker='o', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, marker='o', label='Validation Accuracy')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training vs Validation Accuracy Over Epochs\")\n",
    "    plt.xticks(epochs)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy_per_epoch(train_accuracies, validation_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "866039a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Click here to claim your prize.\n",
      "Predicted class: ham, Probabilities: [0.5060316  0.49396846]\n",
      "\n",
      "Message: Hey, are we still on for lunch tomorrow?\n",
      "Predicted class: ham, Probabilities: [0.84595555 0.15404445]\n",
      "\n",
      "Message: Free entry in a weekly competition to win FA Cup final tickets. Text WIN to 12345 to enter.\n",
      "Predicted class: spam, Probabilities: [0.29880923 0.70119077]\n",
      "\n",
      "Message: Don't forget to pick up groceries on your way home.\n",
      "Predicted class: ham, Probabilities: [0.829075   0.17092505]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a classify function for inference on new text messages\n",
    "\n",
    "def classify_text(model, text, tokenizer, device, max_length):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    input_ids = input_ids[:max_length]\n",
    "    input_ids += [50256] * (max_length - len(input_ids))  # Pad with endoftext token ID\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    logits = model(input_tensor)[:, -1, :]  # Get logits for the last token\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    return predicted_class, probabilities.squeeze().detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# Test the classify_text function\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Click here to claim your prize.\",\n",
    "    \"Hey, are we still on for lunch tomorrow?\",\n",
    "    \"Free entry in a weekly competition to win FA Cup final tickets. Text WIN to 12345 to enter.\",\n",
    "    \"Don't forget to pick up groceries on your way home.\"\n",
    "]\n",
    "for message in test_messages:\n",
    "    predicted_class, probabilities = classify_text(\n",
    "        model = model,\n",
    "        text = message,\n",
    "        tokenizer = tokenizer,\n",
    "        device = device,\n",
    "        max_length = train_dataset.max_length\n",
    "    )\n",
    "    label = \"spam\" if predicted_class == 1 else \"ham\"\n",
    "    print(f\"Message: {message}\\nPredicted class: {label}, Probabilities: {probabilities}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "459e38d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_classify_text(text):\n",
    "    predicted_class, probabilities = classify_text(\n",
    "        model = model,\n",
    "        text = text,\n",
    "        tokenizer = tokenizer,\n",
    "        device = device,\n",
    "        max_length = train_dataset.max_length\n",
    "    )\n",
    "    label = \"spam\" if predicted_class == 1 else \"ham\"\n",
    "    return { \"ham\": float(probabilities[0]), \"spam\": float(probabilities[1]) }\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_classify_text,\n",
    "    inputs=gr.Textbox(lines=4, placeholder=\"Enter a text message here...\"),\n",
    "    outputs=gr.Label(num_top_classes=2, label=\"Predicted Class\"),\n",
    "    title=\"Spam Classifier using Fine-tuned GPT Model\",\n",
    "    description=\"Enter a text message to classify it as 'spam' or 'ham' using the fine-tuned GPT model.\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
