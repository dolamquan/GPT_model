{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0e05d6",
   "metadata": {},
   "source": [
    "# Implementing a GPT model from scratch to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5b64e",
   "metadata": {},
   "source": [
    "## Coding an LLM architecture\n",
    "\n",
    "LLMs, such as GPT, are large deep neural network architectures designed to generate new text one word (or token) at a time. However, despite their size, the model architecture is less complicated than you might think, since many of its components are repeated.\n",
    "\n",
    "![](pic1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ea9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124 = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size of the model\n",
    "    \"context_length\": 1024, # Maximum context length for input sequences\n",
    "    \"embed_dim\": 768, # Dimensionality of the token embeddings\n",
    "    \"n_heads\": 12, # Number of attention heads in the multi-head attention mechanism\n",
    "    \"n_layers\": 12, # Number of transformer layers in the model\n",
    "    \"dropout\": 0.1, # Dropout rate for regularization\n",
    "    \"qkv_bias\": False, # Whether to include bias terms in the query, key, value projections\n",
    "}\n",
    "\n",
    "\n",
    "# Using this configuration, we will implement a GPT placeholder architecture (DummyGPTModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62958ed5",
   "metadata": {},
   "source": [
    "![](pic2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1382acc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jackd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Developed a GPT placeholder model to see the overall structure of the model -> A placeholder GPT backbone\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Defines a pytorch module that behaves like a GPT model\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,config): # Constructor takes a config dictionary\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config['vocab_size'], config['embed_dim']) # Token embedding Layer: Maps each token id to a vector of size embed_dim\n",
    "        self.pos_emb = nn.Embedding(config['context_length'], config['embed_dim']) # Positional embedding Layer: Adds positional information to the token embeddings\n",
    "        self.drop = nn.Dropout(config['dropout']) # Dropout Layer: Regularization to prevent overfitting\n",
    "\n",
    "        # A stack of n_layers transformer blocks wrapped in nn.Sequential\n",
    "        # *[...]: unpacks the list so Sequential receives each block as a separate module\n",
    "        # nn.Sequential: Is a container that lets you build a neural network layer by layer, in order, without having to manually define the forward function\n",
    "        # Think of it like stacking blocks - whatever you put inside will be applied in sequence when data passes through it\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(config) for _ in range(config['n_layers'])] \n",
    "        ) # Creates several transformer layers and stacks them together -> Each DummyTransformerBlock would normally do attention and learning, but right now\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(config['embed_dim']) # Final normalization layer\n",
    "        self.out_head = nn.Linear(config['embed_dim'], config['vocab_size'], bias=False) # Output layer: Maps the final hidden states to vocabulary size for predicting the next token\n",
    "    \n",
    "\n",
    "    # Compute Logits for a batch of token id sequences\n",
    "    # Tells PyTorch how to compute outputs (logits) from the inputs (token IDs)\n",
    "    def forward(self, in_idx):\n",
    "        # in_idx = a batch of tokenized text\n",
    "        # batch_size: number of sequences in the batch\n",
    "        # seq_length: length of each sequence\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # Each token ID is converted into a dense vector of size embed_dim\n",
    "        # This \"embedding\" represents the meaning of that token\n",
    "        tok_embeddings = self.tok_emb(in_idx)\n",
    "\n",
    "        # GPTs can't understand order unless you tell them where each token sits in the sequence\n",
    "        # So we create positional embeddings that add this information\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_length, device=in_idx.device))\n",
    "\n",
    "        # Add token meaning + positional info together, apply dropout for regularization\n",
    "        x = self.drop(tok_embeddings + pos_embeds)\n",
    "\n",
    "        # Normally this is where:\n",
    "        # Self-attention happens (each token \"looks\" at each others)\n",
    "        # Feed-forward networks apply non-linear transformations\n",
    "        # Residual connections and LayerNorms help stability\n",
    "        x = self.trf_blocks(x) # This line sends the embeddings through your stack of transformer layers\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)  # Final linear layer to get logits for each token in the vocabulary\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Logits are the model's raw predictions before applying softmax\n",
    "# They represent unnormalized scores for each token in the vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13d4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text 1: [15496, 703, 389, 345]\n",
      "Tokenized Text 2: [464, 2068, 7586, 21831]\n",
      "tensor([[15496,   703,   389,   345],\n",
      "        [  464,  2068,  7586, 21831]])\n"
     ]
    }
   ],
   "source": [
    "# Visualize how the data flows in and out of the DummyGPTModel\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Load GPT-2 tokenizer\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Hello how are you\"\n",
    "txt2 = \"The quick brown fox\"\n",
    "\n",
    "tokenized_txt1 = tokenizer.encode(txt1)\n",
    "tokenized_txt2 = tokenizer.encode(txt2)\n",
    "\n",
    "print(\"Tokenized Text 1:\", tokenized_txt1)\n",
    "print(\"Tokenized Text 2:\", tokenized_txt2)\n",
    "\n",
    "batch.append(torch.tensor(tokenized_txt1, dtype=torch.long))\n",
    "batch.append(torch.tensor(tokenized_txt2, dtype=torch.long))\n",
    "\n",
    "batch = torch.stack(batch, dim=0) # Create a batch of tokenized inputs\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7758459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1863, -0.5487,  0.4876,  ..., -0.0398,  0.4186, -0.1547],\n",
      "         [-0.6549,  1.8672, -0.9793,  ...,  1.0893,  0.2605,  1.1283],\n",
      "         [ 0.1145, -0.2057,  1.2119,  ...,  0.1425,  1.5854, -0.6929],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[ 0.9693,  0.3007, -0.5556,  ..., -0.5141,  0.6967,  1.0598],\n",
      "         [ 0.4447,  0.3113,  0.0609,  ..., -0.5758,  1.2218,  1.2137],\n",
      "         [ 0.6837,  1.3075,  1.0656,  ...,  1.5503,  0.6306,  0.5630],\n",
      "         [ 0.0194,  0.0736, -0.5092,  ...,  2.1417, -0.3339,  0.3202]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # For reproducibility\n",
    "model = DummyGPTModel(GPT_CONFIG_124) # Instantiate the DummyGPTModel with the GPT-2 configuration\n",
    "logits = model(batch) # Forward pass: Compute logits from the input batch\n",
    "print(\"Logits shape:\", logits.shape) # Should be (batch_size, seq_length, vocab_size)\n",
    "print(logits)\n",
    "\n",
    "# Logits are the model's raw predictions before applying softmax\n",
    "# They represent unnormalized scores for each token in the vocabulary\n",
    "# They indicate how likely each token is to be the next token in the sequence\n",
    "# Higher logits -> higher probability after softmax along the last dimension\n",
    "# 50257 is the vocab size for GPT-2\n",
    "# For every position in each sequence, you have a vector of 50257 logits (one raw score per token in the vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c9c28",
   "metadata": {},
   "source": [
    "### Normalizing Activations With Layer Normalization\n",
    "\n",
    "- Training deep neural networks can be challenging since there are problems such as vanishing or exploding gradients. These problems lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights --> Which means the learning process struggles to find a set of parameters for the neural network that minimizes the loss function\n",
    "- The network has difficulty learning the underlying patterns in the data to a degree that would allow it to make accurate predictions or decisions\n",
    "\n",
    "\n",
    "- Therefore, we need a *layer normalization* to improve the stability and efficiency of neural network training. The main idea behind layer normalization is to adjust the activations of a neural network layer to have a mean of 0 and a variance of 1 -> unit variance\n",
    "- Layer normalization is typically applied before and after the multi-head attention module\n",
    "\n",
    "![](pic3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b4776d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3654, 0.4032])\n",
      "tensor([0.0575, 0.1322])\n",
      "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([0.2432, 0.1928], grad_fn=<MeanBackward1>)\n",
      "tensor([0.0799, 0.0670], grad_fn=<VarBackward0>)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=6, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "mean = out.mean(dim=1, keepdim=True)\n",
    "var = out.var(dim=1, keepdim=True)\n",
    "\n",
    "print(batch_example.mean(dim=1))\n",
    "print(batch_example.var(dim=1))\n",
    "print(layer(batch_example))\n",
    "\n",
    "# Mean\n",
    "print(layer(batch_example).mean(dim=1))\n",
    "# Variance\n",
    "print(layer(batch_example).var(dim=1))\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9edaf61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Layer Norm - Mean: tensor([[0.0000e+00],\n",
      "        [9.9341e-09]], grad_fn=<MeanBackward1>)\n",
      "After Layer Norm - Variance: tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply layer normalization to the layer outputs we obtained earlier \n",
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=1, keepdim=True)\n",
    "var = out_norm.var(dim=1, keepdim=True)\n",
    "\n",
    "print(\"After Layer Norm - Mean:\", mean)\n",
    "print(\"After Layer Norm - Variance:\", var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stabilizes training by keeping activations in a consistent range\n",
    "# Transformers use it heavily:\n",
    "# Before self-attention and feedforward layers\n",
    "# Or after these layers, depending on the architecture\n",
    "\n",
    "\n",
    "# Keeps activations stable by normalizing each token's fetures (embedding vector) so they have mean 0 and variance 1\n",
    "# This helps gradients flow better and prevents exploding/vanishing gradients\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # Small constant to prevent division by zero\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # Learnable scaling parameter - initialized to ones\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c53f65c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After LayerNorm class - Mean: tensor([[ 0.0000e+00],\n",
      "        [-7.1526e-08]], grad_fn=<MeanBackward1>)\n",
      "After LayerNorm class - Variance: tensor([[0.9998],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "In = LayerNorm(emb_dim=5)\n",
    "out_In = In(batch_example)\n",
    "\n",
    "\n",
    "\n",
    "mean = out_In.mean(dim=1, keepdim=True)\n",
    "var = out_In.var(dim=1, keepdim=True)\n",
    "print(\"After LayerNorm class - Mean:\", mean)\n",
    "print(\"After LayerNorm class - Variance:\", var)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f347ef3",
   "metadata": {},
   "source": [
    "### Implementing a feed forward network with GELU activations\n",
    "\n",
    "The approximate GELU function is given by:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2325401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97be4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69f64f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.4\n",
      "PyTorch built with:\n",
      "  - C++ Version: 201703\n",
      "  - MSVC 194234444\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2025.2-Product Build 20250620 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=0fabc3ba44823f257e70ce397d989c8de5e362c1, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/pytorch/.ci/pytorch/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.9.0, USE_CUDA=0, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n",
    "import torch\n",
    "print(torch.__config__.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c3edbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GELU (tanh approximation)\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        c = math.sqrt(2.0 / math.pi)\n",
    "        return 0.5 * x * (1 + torch.tanh(c * (x + 0.044715 * x**3)))\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, steps=200)\n",
    "y_gelu = gelu(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], ['GELU', 'ReLU']), start=1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    # Avoid NumPy: pass Python lists to matplotlib\n",
    "    plt.plot(x.detach().cpu().tolist(), y.detach().cpu().tolist())\n",
    "    plt.title(f'{label} Activation Function')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc6f7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GELU function to implement a feed forward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creates a sequence of layers that make up the feed forward block\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['embed_dim'], cfg['embed_dim'] * 4),\n",
    "            GELU(),\n",
    "            nn.Linear(cfg['embed_dim'] * 4, cfg['embed_dim']),\n",
    "        )\n",
    "    \n",
    "    # Passsses the input through the feed forward layers\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c387af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward Network Output Shape: torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124)\n",
    "x = torch.rand(2,3,768)\n",
    "out_ffn = ffn(x)\n",
    "print(\"Feed Forward Network Output Shape:\", out_ffn.shape)\n",
    "\n",
    "# The FeedForwards module plays a crucial role in enhancing the model's ability\n",
    "# to learn from and generalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d148d20",
   "metadata": {},
   "source": [
    "### Adding Shortcut Connections\n",
    "\n",
    "- Were proposed for deep networks in computer vision to mitigate the challenge of vanishing gradients\n",
    "- The vanishing gradient problem refers to the issue where gradients (which guid weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers\n",
    "\n",
    "![](pic4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d2c47",
   "metadata": {},
   "source": [
    "- Shortcut connection creates an alternative, shorter path for the gradient to flow through the network by skipping one or more layers, which is achieved by adding the output of one layer to the output of a later layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7080090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom deep feedforward neural network implementation with optimal residual connections\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output  # Add shortcut connection\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "317dbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1291e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function that computes the gradients in the model's backward pass\n",
    "\n",
    "def print_gradients(model,x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"Gradient for {name} has gradient mean of {param.grad.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17a0fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for layers.0.0.weight has gradient mean of 6.4675179274609196e-12\n",
      "Gradient for layers.1.0.weight has gradient mean of -8.115144009934738e-05\n",
      "Gradient for layers.2.0.weight has gradient mean of -0.0011454185005277395\n",
      "Gradient for layers.3.0.weight has gradient mean of 0.0027977479621767998\n",
      "Gradient for layers.4.0.weight has gradient mean of 0.0013724254677072167\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af39d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for layers.0.0.weight has gradient mean of -1.6556845894299954e-09\n",
      "Gradient for layers.1.0.weight has gradient mean of 0.007602863013744354\n",
      "Gradient for layers.2.0.weight has gradient mean of -0.008172199130058289\n",
      "Gradient for layers.3.0.weight has gradient mean of 0.11819545179605484\n",
      "Gradient for layers.4.0.weight has gradient mean of 0.8907492756843567\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84d968",
   "metadata": {},
   "source": [
    "### Connecting Attention and Linear Layers in a Transformer Block\n",
    "\n",
    "- Implement the transformer block, a fundamental building block of GPT and other LLM architectures\n",
    "- This block, which is repeated a dozen times in the 124-million-parameter GPT-2 architecture, combines with multi-head attention, layer normalization, dropout, feed forward layers, and GELU activations\n",
    "- After creating the transformer block, we will be connecting it back to the remaining parts of the GPT architecture\n",
    "\n",
    "\n",
    "**Transformer Block:** The Thinking Layer\n",
    "- Is the core layer used in models like BERT and GPT\n",
    "- Each block takes a sequence of token embeddings and return a sequence of the same shape, after mixing information across positions and passing it through a small MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from chapter 3\n",
    "\n",
    "\n",
    "class MultiHeadAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias =False):\n",
    "        # Calls the constructor of the parent class nn.Module\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Dimension of each attention head -> features per head\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Contain the combined results from all attention heads\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape to (batch_size, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = attn_weights @ values\n",
    "\n",
    "        context_vecs = context_vecs.transpose(1,2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        output = self.out_proj(context_vecs)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a25a00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each block handles self-attention, feed forward layers, layer normalization, dropout, and residual connections\n",
    "# MLP: Multi-Layer Perceptron - a small feed-forward neural network: A stack of fully connected\n",
    "# linear layers with non linear activations in between\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creates a multi-head attention layer that allows the model to \"look at\" different parts of the input sequence simultaneously\n",
    "        # All parameters are derived from the cfg dictionary\n",
    "        # It outputs representations of tokens that have contextual meaning\n",
    "        self.att = MultiHeadAttention_v2(\n",
    "            d_in=cfg['embed_dim'],\n",
    "            d_out=cfg['embed_dim'],\n",
    "            context_length=cfg['context_length'],\n",
    "            dropout=cfg['dropout'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "\n",
    "        # Feed forward and Normalization layers\n",
    "        self.ff = FeedForward(cfg) # A simple MLP applied to each token after attention\n",
    "        self.norm1 = LayerNorm(cfg['embed_dim']) # Normalizes features to stabilize training\n",
    "        self.norm2 = LayerNorm(cfg['embed_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['dropout']) # Regularization to prevent overfitting\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Self-attention sub-layer with residual connection\n",
    "        # 1. Save the input for the residual connection\n",
    "        # 2. Normalaize it -> pass it through attention to get attention weights\n",
    "        # 3. Apply dropout for regularization\n",
    "        # 4. Add the original input (shortcut) back to the output of attention\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2683fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2961, 0.5166, 0.2517,  ..., 0.9541, 0.8567, 0.4604],\n",
      "         [0.2238, 0.3047, 0.3019,  ..., 0.5465, 0.4532, 0.7598],\n",
      "         [0.6945, 0.2478, 0.4111,  ..., 0.8838, 0.4898, 0.5963],\n",
      "         [0.0890, 0.7804, 0.9223,  ..., 0.4507, 0.6357, 0.5833]],\n",
      "\n",
      "        [[0.5716, 0.9297, 0.3396,  ..., 0.0477, 0.4564, 0.2797],\n",
      "         [0.0936, 0.2211, 0.3806,  ..., 0.3948, 0.4545, 0.4536],\n",
      "         [0.6788, 0.1741, 0.2084,  ..., 0.5557, 0.5930, 0.0959],\n",
      "         [0.3894, 0.4083, 0.0662,  ..., 0.9861, 0.9341, 0.1319]]])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "tensor([[[-0.0053,  0.0976, -0.1119,  ...,  1.2888,  0.2626,  0.6683],\n",
      "         [ 0.0028, -0.2366,  0.1721,  ...,  0.5953,  0.2498,  0.7447],\n",
      "         [ 0.4675,  0.4470,  0.1792,  ...,  1.2521,  0.3048,  0.7748],\n",
      "         [ 0.0664,  0.7225,  0.9206,  ...,  0.4790,  0.7428,  0.7014]],\n",
      "\n",
      "        [[ 0.3623,  1.2142,  0.5221,  ...,  0.1853,  0.0114, -0.5029],\n",
      "         [-0.0224,  0.7787,  0.2769,  ...,  0.1735,  0.5418,  0.1144],\n",
      "         [ 0.7427,  0.4012,  0.3209,  ...,  0.3268,  0.7522, -0.1639],\n",
      "         [ 0.5743,  0.6240,  0.4408,  ...,  1.1961,  1.2648,  0.2242]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing transformer block\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2,4,768)\n",
    "print(x)\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124)\n",
    "out_block = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out_block.shape)\n",
    "print(out_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa113a9",
   "metadata": {},
   "source": [
    "### Assemble Everything into the GPT Model\n",
    "![](pic5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6dfd2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop = nn.Dropout(cfg['dropout'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        tok_embeddings = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_length, device=in_idx.device))\n",
    "\n",
    "        x = self.drop(tok_embeddings + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc168e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   703,   389,   345],\n",
      "        [  464,  2068,  7586, 21831]])\n",
      "Logits shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1716, -0.0959,  0.2765,  ..., -0.1463,  0.4959, -0.4830],\n",
      "         [-0.2893, -0.4382, -0.3006,  ...,  0.7259,  0.1451, -0.4303],\n",
      "         [ 0.6534,  0.1764, -0.0896,  ..., -0.0177, -0.0242, -0.1244],\n",
      "         [-0.8355,  0.4110, -0.1856,  ...,  0.7248,  0.3003, -0.1707]],\n",
      "\n",
      "        [[-0.9079, -0.6797, -0.3012,  ...,  0.5028,  0.4142, -0.9061],\n",
      "         [-0.2225,  0.0552, -0.1529,  ..., -0.4726, -0.1914,  0.0856],\n",
      "         [ 0.9021,  0.1483,  0.0919,  ...,  0.6109, -0.7892, -0.3287],\n",
      "         [-0.6954, -0.3157, -0.4043,  ...,  0.5993, -0.2863,  0.0094]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124)\n",
    "logits = model(batch)\n",
    "print(batch)\n",
    "\n",
    "# For every position in each sequence, the model outputs a vector of length 50257 (one score per vocab token) predicting the next token at that position\n",
    "# The values are unomralized scores --> Higher logits => higher predicted probability after softmax\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62105b56",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "![](pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "05d6d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model: GPTModel\n",
    "# idx: input token IDs\n",
    "# max_new_tokens: number of new tokens to generate\n",
    "# context_size: maximum context size for the model\n",
    "\n",
    "def generate_text(model,idx,max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop context if needed\n",
    "\n",
    "        # Feed the current sequence to the model to get predictions for the next token\n",
    "        # torch.no_grad(): disables gradient calculation to save memory and computation during inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "    \n",
    "        # We only care about the logits of the last token position, because that's the model's prediction for the next token\n",
    "        logits = logits[:, -1, :]  # Focus on the last token's logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)  # Sample the next token -> can use torch.argmax \n",
    "        idx = torch.cat((idx, next_token), dim=1)  # Append the sampled token to the sequence\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b51aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded start context: [15496, 11, 314, 716]\n",
      "Encoded tensor shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "enconded = tokenizer.encode(start_context)\n",
    "print(\"Encoded start context:\", enconded)\n",
    "\n",
    "encoded_tensor = torch.tensor(enconded).unsqueeze(0)  # Add batch dimension\n",
    "print(\"Encoded tensor shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d817f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token IDs: tensor([[15496,    11,   314,   716,  8164, 44896, 26923, 40578, 16352, 44297]])\n",
      "Output Length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "out = generate_text(model = model\n",
    "                    ,idx = encoded_tensor,\n",
    "                    max_new_tokens = 6,\n",
    "                    context_size = GPT_CONFIG_124['context_length'])\n",
    "\n",
    "print(\"Generated token IDs:\", out)\n",
    "print(\"Output Length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bc396080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Hello, I amClick furiously Connectionposted riot Dek\n"
     ]
    }
   ],
   "source": [
    "decoded_output = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(\"Generated Text:\", decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
